{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "serious-classic",
   "metadata": {},
   "source": [
    "# Pre-train ELECTRA\n",
    "In this section, we will train ELECTRA from scratch with TensorFlow using scripts provided by ELECTRA’s authors in google-research/electra. Then we will convert the model to PyTorch’s checkpoint, which can be easily fine-tuned on downstream tasks using Hugging Face’s transformers library.\n",
    "\n",
    "### Setup\n",
    "!pip install tensorflow-gpu==1.15\n",
    "\n",
    "!pip install transformers==2.8.0\n",
    "\n",
    "!pip install -U tensorboard\n",
    "\n",
    "!git clone https://github.com/google-research/electra.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "engaged-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-payment",
   "metadata": {},
   "source": [
    "### **Data**\n",
    "\n",
    "We will pre-train ELECTRA on a Portuguese movie subtitle dataset retrieved from OpenSubtitles concatened with Wikipedia in Portuguese and BrWac corpus. This dataset is 5.4 GB in size and we will train on a small subset of ~30 MB for presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "southeast-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will pre-train ELECTRA on a Portuguese Wikipedia dataset retrieved from https://dumps.wikimedia.org/. This dataset is 1.8 GB in size.\n",
    "DATA_DIR = 'data/'\n",
    "MODEL_DIR = DATA_DIR + 'models/'\n",
    "MODEL_NAME = 'electranez-small'\n",
    "VOCAB_FILE = DATA_DIR + 'vocab/vocab.txt'\n",
    "TEXT_DIR = DATA_DIR + 'txt/'\n",
    "TFRECORDS_DIR = DATA_DIR + 'pretrain_tfrecords/'\n",
    "\n",
    "if not os.path.exists(VOCAB_FILE):\n",
    "    print('Vocab file not found!')\n",
    "    \n",
    "if not os.path.exists(TEXT_DIR + 'text.txt'):\n",
    "    print('Corpus not found!')\n",
    "    \n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR, mode=0o777)\n",
    "    \n",
    "if not os.path.exists(TFRECORDS_DIR):\n",
    "    os.makedirs(TFRECORDS_DIR, mode=0o777)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-aircraft",
   "metadata": {},
   "source": [
    "### **Start Training**\n",
    "We use run_pretraining.py to pre-train an ELECTRA model.\n",
    "\n",
    "This takes slightly over 4 days on a Tesla V100 GPU. However, the model should achieve decent results after 200k steps (10 hours of training on the v100 GPU).\n",
    "\n",
    "To customize the training, create a .json file containing the hyperparameters. Please refer configure_pretraining.py for default values of all hyperparameters.\n",
    "\n",
    "Below, we set the hyperparameters to train the model for only 100 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-discipline",
   "metadata": {},
   "source": [
    "**We use build_pretraining_dataset.py to create a pre-training dataset from a dump of raw text.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "comparable-official",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"../../google/electra/build_pretraining_dataset.py\", line 230, in <module>\n",
      "    main()\n",
      "  File \"../../google/electra/build_pretraining_dataset.py\", line 216, in main\n",
      "    utils.rmkdir(args.output_dir)\n",
      "  File \"/home/modanez/Documentos/AmbienteDeTrabalho/Desenvolvimento/python/google/electra/util/utils.py\", line 65, in rmkdir\n",
      "    mkdir(path)\n",
      "  File \"/home/modanez/Documentos/AmbienteDeTrabalho/Desenvolvimento/python/google/electra/util/utils.py\", line 55, in mkdir\n",
      "    tf.io.gfile.makedirs(path)\n",
      "  File \"/home/modanez/anaconda3/envs/electra/lib/python3.7/site-packages/tensorflow_core/python/lib/io/file_io.py\", line 453, in recursive_create_dir_v2\n",
      "    pywrap_tensorflow.RecursivelyCreateDir(compat.as_bytes(path))\n",
      "tensorflow.python.framework.errors_impl.PermissionDeniedError: /pretrain_tfrecords; Permission denied\n",
      "Duration: 0:00:01.286837\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "!python3 ../../google/electra/build_pretraining_dataset.py \\\n",
    "  --corpus-dir $DATA_DIR/txt/ \\\n",
    "  --vocab-file $VOCAB_DIR/vocab.txt \\\n",
    "  --output-dir $DATA_DIR/pretrain_tfrecords/ \\\n",
    "  --max-seq-length 128 \\\n",
    "  --blanks-separate-docs False \\\n",
    "  --no-lower-case \\\n",
    "  --num-processes 10\n",
    "\n",
    "print('Duration: {}'.format(datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "signal-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'do_train': 'true',\n",
    "    'do_eval': 'false',\n",
    "    'model_size': 'small',\n",
    "    'do_lower_case': 'false',\n",
    "    'vocab_size': 52000,\n",
    "    'num_train_steps': 10000,\n",
    "    'save_checkpoints_steps': 1000,\n",
    "    'train_batch_size': 36,\n",
    "    'electra_objective' :  True\n",
    "}\n",
    "\n",
    "with open(DATA_DIR + 'hparams_36.json', 'w') as f:\n",
    "    json.dump(hparams, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "constant-breed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Config:\n",
      "================================================================================\n",
      "debug False\n",
      "disallow_correct False\n",
      "disc_weight 50.0\n",
      "do_eval false\n",
      "do_lower_case false\n",
      "do_train true\n",
      "electra_objective True\n",
      "electric_objective False\n",
      "embedding_size 128\n",
      "eval_batch_size 128\n",
      "gcp_project None\n",
      "gen_weight 1.0\n",
      "generator_hidden_size 0.25\n",
      "generator_layers 1.0\n",
      "iterations_per_loop 200\n",
      "keep_checkpoint_max 5\n",
      "learning_rate 0.0005\n",
      "lr_decay_power 1.0\n",
      "mask_prob 0.15\n",
      "max_predictions_per_seq 19\n",
      "max_seq_length 128\n",
      "model_dir data/models/electranez-small\n",
      "model_hparam_overrides {}\n",
      "model_name electranez-small\n",
      "model_size small\n",
      "num_eval_steps 100\n",
      "num_tpu_cores 1\n",
      "num_train_steps 10000\n",
      "num_warmup_steps 10000\n",
      "pretrain_tfrecords data/pretrain_tfrecords/pretrain_data.tfrecord*\n",
      "results_pkl data/models/electranez-small/results/unsup_results.pkl\n",
      "results_txt data/models/electranez-small/results/unsup_results.txt\n",
      "save_checkpoints_steps 1000\n",
      "temperature 1.0\n",
      "tpu_job_name None\n",
      "tpu_name None\n",
      "tpu_zone None\n",
      "train_batch_size 36\n",
      "two_tower_generator False\n",
      "uniform_generator False\n",
      "untied_generator True\n",
      "untied_generator_embeddings False\n",
      "use_tpu False\n",
      "vocab_file data/vocab.txt\n",
      "vocab_size 52000\n",
      "weight_decay_rate 0.01\n",
      "\n",
      "================================================================================\n",
      "Running training\n",
      "================================================================================\n",
      "Traceback (most recent call last):\n",
      "  File \"../../google/electra/run_pretraining.py\", line 469, in <module>\n",
      "    main()\n",
      "  File \"../../google/electra/run_pretraining.py\", line 465, in main\n",
      "    args.model_name, args.data_dir, **hparams))\n",
      "  File \"../../google/electra/run_pretraining.py\", line 427, in train_or_eval\n",
      "    estimator.train(input_fn=pretrain_data.get_input_fn(config, True),\n",
      "  File \"/home/modanez/Documentos/AmbienteDeTrabalho/Desenvolvimento/python/google/electra/pretrain/pretrain_data.py\", line 38, in get_input_fn\n",
      "    input_files.extend(tf.io.gfile.glob(input_pattern))\n",
      "  File \"/home/modanez/anaconda3/envs/electra/lib/python3.7/site-packages/tensorflow_core/python/lib/io/file_io.py\", line 384, in get_matching_files_v2\n",
      "    compat.as_bytes(pattern))\n",
      "tensorflow.python.framework.errors_impl.NotFoundError: data/pretrain_tfrecords; No such file or directory\n",
      "Duration: 0:00:01.577493\n"
     ]
    }
   ],
   "source": [
    "# Let’s start training:\n",
    "start_time = datetime.now()\n",
    "\n",
    "!python3 ../../google/electra/run_pretraining.py --data-dir $DATA_DIR --model-name $MODEL_NAME --hparams $DATA_DIR'hparams_36.json'\n",
    "\n",
    "print('Duration: {}'.format(datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-pursuit",
   "metadata": {},
   "source": [
    "If you are training on a virtual machine, run the following lines on the terminal to moniter the training process with TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "superb-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-lexington",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "imperial-burst",
   "metadata": {},
   "source": [
    "### **Convert Tensorflow checkpoints to PyTorch format**\n",
    "\n",
    "Hugging Face has a tool to convert Tensorflow checkpoints to PyTorch. However, this tool has yet been updated for ELECTRA. Fortunately, I found a GitHub repo by [@lonePatient](https://github.com/lonePatient/electra_pytorch.git) that can help us with this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "likely-cooper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "atlantic-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  'vocab_size': 119547,\n",
    "  'embedding_size': 128,\n",
    "  'hidden_size': 256,\n",
    "  'num_hidden_layers': 12,\n",
    "  'num_attention_heads': 4,\n",
    "  'intermediate_size': 1024,\n",
    "  'generator_size': '0.25',\n",
    "  'hidden_act': 'gelu',\n",
    "  'hidden_dropout_prob': 0.1,\n",
    "  'attention_probs_dropout_prob': 0.1,\n",
    "  'max_position_embeddings': 512,\n",
    "  'type_vocab_size': 2,\n",
    "  'initializer_range': 0.02\n",
    "}\n",
    "\n",
    "with open(MODEL_DIR + '/config.json', 'w') as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "potential-astrology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/models/electranez-small/\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "final-clerk",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:model.configuration_utils:loading configuration file data/models/electranez-small//config.json\n",
      "INFO:model.configuration_utils:Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"embedding_size\": 128,\n",
      "  \"finetuning_task\": null,\n",
      "  \"generator_size\": \"0.25\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1024,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 4,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "INFO:model.modeling_electra:Converting TensorFlow checkpoint from /home/modanez/Documentos/AmbienteDeTrabalho/Desenvolvimento/python/modanez/electra/data/models/electranez-small\n",
      "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense_1/bias with shape [1]\n",
      "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense_1/bias/adam_m with shape [1]\n",
      "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense_1/bias/adam_v with shape [1]\n",
      "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense_1/kernel with shape [256, 1]\n",
      "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense_1/kernel/adam_m with shape [256, 1]\n",
      "INFO:model.modeling_electra:Loading TF weight discriminator_predictions/dense_1/kernel/adam_v with shape [256, 1]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/beta with shape [128]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/beta/adam_m with shape [128]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/beta/adam_v with shape [128]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/gamma with shape [128]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/gamma/adam_m with shape [128]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings/LayerNorm/gamma/adam_v with shape [128]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings/position_embeddings with shape [512, 128]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings/position_embeddings/adam_m with shape [512, 128]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings/position_embeddings/adam_v with shape [512, 128]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings/token_type_embeddings with shape [2, 128]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings/token_type_embeddings/adam_m with shape [2, 128]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings/token_type_embeddings/adam_v with shape [2, 128]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings/word_embeddings with shape [119547, 128]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings/word_embeddings/adam_m with shape [119547, 128]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings/word_embeddings/adam_v with shape [119547, 128]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings_project/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings_project/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings_project/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings_project/kernel with shape [128, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings_project/kernel/adam_m with shape [128, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/embeddings_project/kernel/adam_v with shape [128, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/key/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/query/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/attention/self/value/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/bias with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/bias/adam_m with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/bias/adam_v with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/kernel with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/kernel with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/kernel/adam_m with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_0/output/dense/kernel/adam_v with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/key/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/query/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/attention/self/value/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/bias with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/bias/adam_m with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/bias/adam_v with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/kernel with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/kernel with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/kernel/adam_m with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_1/output/dense/kernel/adam_v with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/key/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/query/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/attention/self/value/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/bias with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/bias/adam_m with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/bias/adam_v with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/kernel with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/kernel with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/kernel/adam_m with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_10/output/dense/kernel/adam_v with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/key/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/query/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/attention/self/value/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/bias with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/bias/adam_m with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/bias/adam_v with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/kernel with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/kernel with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/kernel/adam_m with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_11/output/dense/kernel/adam_v with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/key/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/query/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/attention/self/value/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/bias with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/bias/adam_m with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/bias/adam_v with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/kernel with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/kernel with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/kernel/adam_m with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_2/output/dense/kernel/adam_v with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/key/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/query/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/attention/self/value/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/bias with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/bias/adam_m with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/bias/adam_v with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/kernel with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/kernel with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/kernel/adam_m with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_3/output/dense/kernel/adam_v with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/key/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/query/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/attention/self/value/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/bias with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/bias/adam_m with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/bias/adam_v with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/kernel with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/kernel with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/kernel/adam_m with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_4/output/dense/kernel/adam_v with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/key/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/query/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/attention/self/value/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/bias with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/bias/adam_m with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/bias/adam_v with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/kernel with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/kernel with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/kernel/adam_m with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_5/output/dense/kernel/adam_v with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/key/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/query/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/attention/self/value/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/bias with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/bias/adam_m with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/bias/adam_v with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/kernel with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/kernel with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/kernel/adam_m with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_6/output/dense/kernel/adam_v with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/key/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/query/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/attention/self/value/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/bias with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/bias/adam_m with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/bias/adam_v with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/kernel with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/kernel with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/kernel/adam_m with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_7/output/dense/kernel/adam_v with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/key/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/query/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/attention/self/value/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/bias with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/bias/adam_m with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/bias/adam_v with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/kernel with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/kernel with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/kernel/adam_m with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_8/output/dense/kernel/adam_v with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/output/dense/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/key/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/query/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/kernel with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/kernel/adam_m with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/attention/self/value/kernel/adam_v with shape [256, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/bias with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/bias/adam_m with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/bias/adam_v with shape [1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/kernel with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/kernel/adam_m with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/intermediate/dense/kernel/adam_v with shape [256, 1024]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/beta with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/beta/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/beta/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/gamma with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/gamma/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/LayerNorm/gamma/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/kernel with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/kernel/adam_m with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight electra/encoder/layer_9/output/dense/kernel/adam_v with shape [1024, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/embeddings_project/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/embeddings_project/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/embeddings_project/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/embeddings_project/kernel with shape [128, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/embeddings_project/kernel/adam_m with shape [128, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/embeddings_project/kernel/adam_v with shape [128, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/key/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/query/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/attention/self/value/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/kernel with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/kernel with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/kernel/adam_m with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_0/output/dense/kernel/adam_v with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/key/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/query/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/attention/self/value/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/kernel with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/kernel with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/kernel/adam_m with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_1/output/dense/kernel/adam_v with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/key/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/query/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/attention/self/value/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/kernel with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/kernel with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/kernel/adam_m with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_10/output/dense/kernel/adam_v with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/key/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/query/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/attention/self/value/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/kernel with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/kernel with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/kernel/adam_m with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_11/output/dense/kernel/adam_v with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/key/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/query/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/attention/self/value/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/kernel with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/kernel with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/kernel/adam_m with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_2/output/dense/kernel/adam_v with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/key/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/query/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/attention/self/value/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/kernel with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/kernel with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/kernel/adam_m with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_3/output/dense/kernel/adam_v with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/key/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/query/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/attention/self/value/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/kernel with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/kernel with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/kernel/adam_m with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_4/output/dense/kernel/adam_v with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/key/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/query/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/attention/self/value/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/kernel with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/kernel with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/kernel/adam_m with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_5/output/dense/kernel/adam_v with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/key/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/query/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/attention/self/value/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/kernel with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/kernel with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/kernel/adam_m with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_6/output/dense/kernel/adam_v with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/key/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/query/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/attention/self/value/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/kernel with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/kernel with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/kernel/adam_m with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_7/output/dense/kernel/adam_v with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/key/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/query/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/attention/self/value/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/kernel with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/kernel with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/kernel/adam_m with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_8/output/dense/kernel/adam_v with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/output/dense/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/key/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/query/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/kernel with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/kernel/adam_m with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/attention/self/value/kernel/adam_v with shape [64, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/bias with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/bias/adam_m with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/bias/adam_v with shape [256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/kernel with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/kernel/adam_m with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/intermediate/dense/kernel/adam_v with shape [64, 256]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/beta with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/beta/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/beta/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/gamma with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/gamma/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/LayerNorm/gamma/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/bias with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/bias/adam_m with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/bias/adam_v with shape [64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/kernel with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/kernel/adam_m with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator/encoder/layer_9/output/dense/kernel/adam_v with shape [256, 64]\n",
      "INFO:model.modeling_electra:Loading TF weight generator_predictions/LayerNorm/beta with shape [128]\n",
      "INFO:model.modeling_electra:Loading TF weight generator_predictions/LayerNorm/beta/adam_m with shape [128]\n",
      "INFO:model.modeling_electra:Loading TF weight generator_predictions/LayerNorm/beta/adam_v with shape [128]\n",
      "INFO:model.modeling_electra:Loading TF weight generator_predictions/LayerNorm/gamma with shape [128]\n",
      "INFO:model.modeling_electra:Loading TF weight generator_predictions/LayerNorm/gamma/adam_m with shape [128]\n",
      "INFO:model.modeling_electra:Loading TF weight generator_predictions/LayerNorm/gamma/adam_v with shape [128]\n",
      "INFO:model.modeling_electra:Loading TF weight generator_predictions/dense/bias with shape [128]\n",
      "INFO:model.modeling_electra:Loading TF weight generator_predictions/dense/bias/adam_m with shape [128]\n",
      "INFO:model.modeling_electra:Loading TF weight generator_predictions/dense/bias/adam_v with shape [128]\n",
      "INFO:model.modeling_electra:Loading TF weight generator_predictions/dense/kernel with shape [64, 128]\n",
      "INFO:model.modeling_electra:Loading TF weight generator_predictions/dense/kernel/adam_m with shape [64, 128]\n",
      "INFO:model.modeling_electra:Loading TF weight generator_predictions/dense/kernel/adam_v with shape [64, 128]\n",
      "INFO:model.modeling_electra:Loading TF weight generator_predictions/output_bias with shape [119547]\n",
      "INFO:model.modeling_electra:Loading TF weight generator_predictions/output_bias/adam_m with shape [119547]\n",
      "INFO:model.modeling_electra:Loading TF weight generator_predictions/output_bias/adam_v with shape [119547]\n",
      "INFO:model.modeling_electra:Loading TF weight global_step with shape []\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['discriminator_predictions', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping discriminator_predictions/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping discriminator_predictions/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['discriminator_predictions', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping discriminator_predictions/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping discriminator_predictions/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['discriminator_predictions', 'classifier', 'bias']\n",
      "INFO:model.modeling_electra:Skipping discriminator_predictions/classifier/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping discriminator_predictions/classifier/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['discriminator_predictions', 'classifier', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping discriminator_predictions/classifier/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping discriminator_predictions/classifier/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'embeddings', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/embeddings/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/embeddings/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'embeddings', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/embeddings/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/embeddings/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'embeddings', 'position_embeddings']\n",
      "INFO:model.modeling_electra:Skipping electra/embeddings/position_embeddings/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/embeddings/position_embeddings/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'embeddings', 'token_type_embeddings']\n",
      "INFO:model.modeling_electra:Skipping electra/embeddings/token_type_embeddings/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/embeddings/token_type_embeddings/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'embeddings', 'word_embeddings']\n",
      "INFO:model.modeling_electra:Skipping electra/embeddings/word_embeddings/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/embeddings/word_embeddings/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'embeddings_project', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/embeddings_project/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/embeddings_project/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'embeddings_project', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/embeddings_project/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/embeddings_project/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_0/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_1/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_10/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_11/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_2/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_3/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_4/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_5/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_6/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_7/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_8/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping electra/encoder/layer_9/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'embeddings_project', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/embeddings_project/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/embeddings_project/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'embeddings_project', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/embeddings_project/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/embeddings_project/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_0', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_0/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_1', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_1/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_10', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_10/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_11', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_11/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_2', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_2/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_3', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_3/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_4', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_4/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_5', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_5/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_6', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_6/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_7', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_7/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_8', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_8/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/key/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/key/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/key/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/key/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/query/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/query/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/query/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/query/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/value/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/value/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/value/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/attention/self/value/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/intermediate/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/intermediate/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/intermediate/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/intermediate/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/output/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/output/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/output/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/output/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'output', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/output/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/output/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator', 'encoder', 'layer_9', 'output', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/output/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator/encoder/layer_9/output/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator_predictions', 'LayerNorm', 'beta']\n",
      "INFO:model.modeling_electra:Skipping generator_predictions/LayerNorm/beta/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator_predictions/LayerNorm/beta/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator_predictions', 'LayerNorm', 'gamma']\n",
      "INFO:model.modeling_electra:Skipping generator_predictions/LayerNorm/gamma/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator_predictions/LayerNorm/gamma/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator_predictions', 'dense', 'bias']\n",
      "INFO:model.modeling_electra:Skipping generator_predictions/dense/bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator_predictions/dense/bias/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator_predictions', 'dense', 'kernel']\n",
      "INFO:model.modeling_electra:Skipping generator_predictions/dense/kernel/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator_predictions/dense/kernel/adam_v\n",
      "INFO:model.modeling_electra:Initialize PyTorch weight ['generator_predictions', 'output_bias']\n",
      "INFO:model.modeling_electra:Skipping generator_predictions/output_bias/adam_m\n",
      "INFO:model.modeling_electra:Skipping generator_predictions/output_bias/adam_v\n",
      "INFO:model.modeling_electra:Skipping global_step\n",
      "Save PyTorch model to data/models/electranez-small//pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!python ../../diversos/electra_pytorch/convert_electra_tf_checkpoint_to_pytorch.py \\\n",
    "    --tf_checkpoint_path=$MODEL_DIR/electranez-small/ \\\n",
    "    --electra_config_file=$MODEL_DIR/config.json \\\n",
    "    --pytorch_dump_path=$MODEL_DIR/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-warrant",
   "metadata": {},
   "source": [
    "### **Use ELECTRA with transformers**\n",
    "\n",
    "After converting the model checkpoint to PyTorch format, we can start to use our pre-trained ELECTRA model on downstream tasks with the transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "monthly-mailman",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraForPreTraining, ElectraTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pressing-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = ElectraForPreTraining.from_pretrained(MODEL_DIR+'/electranez-small/')\n",
    "tokenizer = ElectraTokenizerFast.from_pretrained(DATA_DIR, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "primary-williams",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [CLS]     os passar   ##os   esta    ##oconversa  ##ndo  [SEP]\n",
      "\n",
      "      1      0      0      0      0      0      0      0      1"
     ]
    }
   ],
   "source": [
    "sentence = 'os pássaros estão cantando' # The birds are singing\n",
    "fake_sentence = 'os pássaros estão conversando' # The birds are speaking \n",
    "\n",
    "fake_tokens = tokenizer.tokenize(fake_sentence, add_special_tokens=True)\n",
    "fake_inputs = tokenizer.encode(fake_sentence, return_tensors='pt')\n",
    "discriminator_outputs = discriminator(fake_inputs)\n",
    "predictions = discriminator_outputs[0] > 0\n",
    "\n",
    "[print('%7s' % token, end='') for token in fake_tokens]\n",
    "print('\\n')\n",
    "[print('%7s' % int(prediction), end='') for prediction in predictions.tolist()];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-greensboro",
   "metadata": {},
   "source": [
    "### **Build Pretraining Dataset**\n",
    "\n",
    "We will use the tokenizer of bert-base-multilingual-cased to process Portuguese texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pretrained WordPiece tokenizer to get vocab.txt\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer.save_pretrained(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-theorem",
   "metadata": {},
   "source": [
    "**To train a small ELECTRA model for 1 million steps, run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-arena",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 ../../google/electra/run_pretraining.py --data-dir $DATA_DIR --model-name $MODEL_NAME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
